<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Interpretability in Reinforcement Learning Agents</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="./writeup.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<h1 id="regularization-and-visualization-of-attention-in-reinforcement-learning-agents">Regularization and visualization of attention in reinforcement learning agents</h1>
<p>Dmitry Nikulin, Sebastian Kosch, Fabian Steuer, Hoagy Cunningham</p>
<p><a href="https://github.com/dniku/rl-attention">Github Repository</a></p>

<p><em>This project was completed during AI Safety Camp 3 in Ávila, Spain, in May 2019. The goal was to familiarize ourselves with the state of the art in visualizing machine learning models, particularly in an reinforcement learning context, and to test possible improvements.</em></p>
<h2 id="introduction">Introduction</h2>
<p>Advances in deep learning are enabling reinforcement learning (RL) agents to accomplish increasingly difficult tasks. For instance, relatively simple machine learning agents can learn how to beat humans in video games, without ever having been programmed how to do so. However, agents sometimes learn to make correct decisions for the wrong reasons, which can lead to surprising and perplexing failures later. In order to diagnose such problems effectively, the developer needs to understand how information flows through the artificial neural network that powers the agent's decision-making process.</p>
<p>One approach to understanding complex models operating on image inputs is through saliency maps. A saliency map is a heatmap highlighting those pixels of the input image that are most responsible for the model output. An example could be a neural network performing an image classification task: given a photograph of a dog in a meadow that was correctly classfied as "dog", a visualization of those pixels the network considers most dog-like provides a check on whether the network has truly learned the concept of dogs, or whether it merely made a lucky guess based on the presence of the meadow.</p>
<p>To some extent, the methods for generating saliency maps can be repurposed for the analysis of RL agents playing video games, since the RL infers actions (labels) from images (render frames). However, the resulting heatmaps are often blurry or noisy. Furthermore, image classifiers simply detect objects, while RL agents must choose actions based on complex relationships between entities detected in the input. A simple heatmap visualization cannot convey whether and where such relationships were detected.</p>
<p>In this work, we present a potential improvement to existing visualization approaches in RL, and report on our experimental findings regarding their performance on six Atari games.</p>
<h2 id="attention-in-rl-agents">Attention in RL agents</h2>
<p>In 2018, <a href="#fn1">Yang et al.</a> explored the effects of adding two <em>attention layers</em> to the decision-making network of an agent learning to play Breakout and other games. The attention layers, applied after some convolutional layers which detect basic game entities, restrict the input to the agent's action selection mechanism to a subset of the input pixels (the <em>attention mask</em>). In effect, the agent's model is forced to focus spatially. Adding such a bottleneck can improve sample efficiency, but more importantly, the attention layer activations provide a direct clue about what the model is focusing on. This directness is attractive when compared to <em>post-hoc</em> methods, which require additional computation to reason about the relevance of network activations after inference.</p>
<p>Note that there is no direct correspondence between activated attention layer neurons and relevant input pixels. This is due to the convolutional downsampling layers that separate the input image from the attention layers. However, we can generate a heatmap by backpropagating the attention tensor through the network. Several different approaches exist to accomplish this, from <a href="#fn2">Simonyan et al.</a>'s gradient method to the more recent VarGrad and SmoothGrad sampling methods.</p>
<p>After some experimentation, <a href="#fn1">Yang et al.</a> chose a simpler approach, where they simply visualized the receptive field of the neuron which corresponded to the strongest activation in their agent's attention layer. Their findings confirm that in trained agents, the attention tends to be strong near crucial entities in the game, i.e. the Pacman sprite or the moving ball in Breakout. However, the attention mask heatmaps are fairly crude.</p>
<h2 id="adding-regularization-to-sharpen-the-attention-masks">Adding regularization to sharpen the attention masks</h2>
<p>The effectiveness of attention layers depends crucially on how the attention is constrained. This is especially true because of the downsampling action of the convolutional layers in <a href="#fn1">Yang et al.</a>'s architecture: a diffuse attention tensor will effectively correspond to all input pixels, defeating the purpose of the attention layer.</p>
<p>To incentivize more informative heatmaps than those obtained by <a href="#fn1">Yang et al.</a>, we added an extra loss term to represent the diffuseness of the attention tensor. Several such measures exist; we settled on using the entropy of the final attention layer.</p>
<div class="figure">
<img src="images/architecture.png" class="center" alt="Diagram of the architecture used in our models." width="600"/>
<p class="caption">The architecture of <a href="#fn1">Yang et al.</a>, which we used in all experiments. The place where we applied entropy loss is highlighted.</p>
</div>

<p>For a discrete probability distribution \(p_i, i=1..n\) entropy is defined as follows:</p>
<p>\[ \operatorname{entropy}(p) = -\sum_{i = 1}^n p_i \cdot \log(p_i). \]</p>
<p>This quantity is greatest when \(p_i \equiv \frac 1 n\) for all \(i\), i.e., when the corresponding probability distribution is very evenly spread. By contrast, it is equal to zero when \(p_i = 1\) for some \(i\), while the other outcomes have zero probability.</p>
<p>In our case, we regard the output of the attention layer as a probability distribution, and we modify the loss in such way as to minimize the entropy of this distribution. Specifically, we add \(\lambda \cdot \operatorname{entropy}(attn)\) to the loss, where \(\lambda\) is a non-negative coefficient and \(attn\) is the output of the attention layer.</p>
<p>Although attention mechanisms have been shown to improve training times, excessively strong regularization will naturally prevent the agent from taking into account complex relationships between spatially distant entities, and thus degrade performance. We ran a suite of experiments to quantify the impact of entropy regularization on the agent's performance at playing Atari games.</p>
<h3 id="experimental-results">Experimental Results</h3>
<p>We recreated the agent by <a href="#fn1">Yang et al.</a> in TensorFlow using the <a href="#fn3"><code>stable-baselines</code></a> package, a fork of OpenAI's <code>baselines</code> package with a stable programming interface and improved documentation. Since <a href="#fn3"><code>stable-baselines</code></a> does not include a full implementation of the Rainbow algorithm, we used PPO (<a href="#fn4">Schulman et al., 2017</a>)`` as another state-of-the-art algorithm available in the library.</p>
<p>Our experiments show that entropy regularization can be added in such way that its effects on attention maps become pronounced, but performance does not suffer noticeably. The following figure shows average reward agents obtain during training with varying \(\lambda\). The value \(\lambda = 0.0\) means that entropy loss did not affect training. This setup should be regarded as baseline.</p>
<div class="figure">
<img src="images/reward_curves.png" alt="Average reward during training" class="center"/>
<p class="caption">Average reward during training</p>
</div>

<p>From the data it is clear that \(\lambda = 0.0005\) does not lead to any drop in performance. The following figure shows impact on the resulting entropy value.</p>
<div class="figure">
<img src="images/scatterplots.png" alt="Scatterplot of final performance. X-axis: attention entropy. Y-axis: average reward." class="center"/>
<p class="caption">Scatterplot of final performance. X-axis: attention entropy. Y-axis: average reward. Solid circles denote individual runs with various random seeds. Cross marks denote averages across runs.</p>
</div>

<p>With the exception for BeamRider, it is clear that for the particular case of these Atari games, it is possible to choose a value of \(\lambda\) such that the extra term in the loss will have a noticeable effect on entropy value, while final performance will not suffer.</p>
<p>BeamRider is different because for these particular training runs, none of the agents achieved good performance. This is expected: in the original PPO paper (<a href="#fn4">Schulman et al., 2017</a>) it is reported that learning only starts after roughly 10M frames (we terminate training after exactly 10M frames).</p>
<p>The following videos show what learned attention maps look like for different \(\lambda\).</p>
<figure class="video_container">
<video controls="true" allowfullscreen="true", width="800">
    <source src="videos/BeamRider.mp4" type="video/webm">
</video>
<video controls="true" allowfullscreen="true", width="800">
    <source src="videos/Breakout.mp4" type="video/webm">
</video>
<video controls="true" allowfullscreen="true", width="800">
    <source src="videos/MsPacman.mp4" type="video/webm">
</video>
<video controls="true" allowfullscreen="true", width="800">
    <source src="videos/Frostbite.mp4" type="video/webm">
</video>
<video controls="true" allowfullscreen="true", width="800">
    <source src="videos/Enduro.mp4" type="video/webm">
</video>
<video controls="true" allowfullscreen="true", width="800">
    <source src="videos/Seaquest.mp4" type="video/webm">
</video>
<p class="caption">Gameplay video for agents trained with different values of \(\lambda\) (left to right: \(\lambda = 0, 0.0005, 0.001, 0.002, 0.003, 0.005\)). In each video, the top row shows the original observations with an attention overlay, and the bottom row shows the observations as received by the neural network after preprocessing. In the attention overlay, each rectangle corresponds to one neuron in the attention layer, and the color intensity is proportional to activation values.</p>
</figure>

<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Yang et al. <a href="https://arxiv.org/pdf/1812.11276.pdf">Learn to Interpret Atari Agents</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Simonyan et al. <a href="https://arxiv.org/abs/1312.6034.pdf">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Hill et al. <a href="https://github.com/hill-a/stable-baselines">Stable Baselines</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Schulman et al. <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a><a href="#fnref4">↩</a></p></li>
</ol>
</div>
</body>
</html>
